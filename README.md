# Data-Science-And-Statistics-Practical

Exp1 – Data Acquisition

Learn how to gather data from various sources such as CSV files, databases, online repositories, or APIs, and load it into Python for analysis.

Exp2 – Central Tendency of Major (Mean, Median, Mode)

Understand and compute measures of central tendency—mean, median, and mode—to summarize and understand the center of a dataset.

Exp3 – Basics of DataFrame

Introduction to Pandas DataFrame: creating DataFrames, accessing rows/columns, basic operations (info, describe), and data manipulation.

Exp4 – Missing Value Treatment

Explore techniques to identify and handle missing data using methods like deletion, mean/median imputation, interpolation, or advanced techniques.

Exp5 – Creation of Arrays (1D, 2D, Multidimensional)

Learn to create and manipulate arrays using NumPy, including 1D, 2D, and multidimensional arrays, along with basic array operations.

Exp6 – Data Visualization

Create visual representations of data using libraries such as Matplotlib or Seaborn—line plots, bar charts, histograms, scatter plots, etc.

Exp7 – Simple Linear Regression for Salary Data

Implement simple linear regression to model the relationship between two variables (e.g., years of experience vs. salary) and make predictions.

Exp8 – Logistic Regression Algorithm

Apply logistic regression for binary classification problems and understand concepts like sigmoid function, decision boundary, and model evaluation.

Exp9 – K-Nearest Neighbor (KNN) Algorithm

Use the KNN algorithm for classification or regression tasks and learn how distance metrics and the choice of k affect performance.

Exp10 – Support Vector Machine (SVM)

Implement SVM for classification, understand hyperplanes, margins, kernel functions, and how SVM separates data into classes.

Exp11 – Decision Tree

Build decision tree models for classification or regression, learn how trees split data, and interpret decision paths.

Exp12 – Random Forest

Use the Random Forest ensemble technique to improve accuracy and reduce overfitting by combining multiple decision trees.
